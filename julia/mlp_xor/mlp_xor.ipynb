{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Zygote\n",
    "using Statistics\n",
    "using Plots\n",
    "using StatsPlots\n",
    "\n",
    "\"\"\"\n",
    "Zygote is used for automatic differentiation\n",
    "Statistics gives us mean()\n",
    "We'll use Plots for a line plot and StatsPlots for a violin plot\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# learning to XOR\n",
    "\n",
    "Hi. This is a tutorial about building a very simple multilayer perceptron to approximate the exclusive-or function, also known as the XOR function to its friends. It might also be your introduction to the Julia programming language. Developed for scientific computing, Julia is ostensibly something of a faster Python. One interesting feature of the language is that when you see a mathematical definition for a dense layer in a neural network, like so:\n",
    "\n",
    "$\n",
    "f(x) = \\sigma(\\theta_w x + b)\n",
    "$\n",
    "\n",
    "You can actually write code that looks very similar, thanks to Julia's support for unicode characters. It doesn't necessarily save you any time typing (symbols are typed by entering the $\\LaTeX$ code, _e.g._ `\\sigma` and pressing tab), but it does look pretty cool. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "σ(x) = 1 ./ (1 .+ exp.(-x))\n",
    "\n",
    "f(x, θ) = σ(x * θ[:w] .+ θ[:b])\n",
    "\n",
    "θ = Dict(:w => randn(32,2)/10, :b => randn(1,2)/100)\n",
    "x = randn(4,32)\n",
    "\n",
    "f(x, θ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions below generate model weights and a noisy dataset representing the XOR function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_xor = function(num_samples=512, dim_x=3)\n",
    "    x = 1*rand(num_samples,dim_x) .> 0.5\n",
    "    y = zeros(num_samples,1) \n",
    "\n",
    "    for ii = 1:size(y)[1]\n",
    "        y[ii] = reduce(xor, x[ii,:])\n",
    "    end\n",
    "\n",
    "    x = x + randn(num_samples,dim_x) / 10\n",
    "\n",
    "    return x, y\n",
    "end\n",
    "\n",
    "init_weights = function(dim_in=2, dim_out=1, dim_hid=4)\n",
    "    \n",
    "    wxh = randn(dim_in, dim_hid) / 8\n",
    "    why = randn(dim_hid, dim_out) / 4\n",
    "    θ = Dict(:wxh => wxh, :why => why)\n",
    "    \n",
    "    return θ\n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next bit defines the model we'll be training: a tiny MLP with 1 hidden layer and no biases. We also need to set up a few helper functions to provide loss and other training metrics (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f(x, θ) = σ(σ(x * θ[:wxh]) * θ[:why])\n",
    "\n",
    "\n",
    "get_accuracy(y, pred, boundary=0.5) = mean(y .== (pred .> boundary)) \n",
    "\n",
    "log_loss = function(y, pred)\n",
    "   \n",
    "    return -(1 / size(y)[1]) .* sum(y .* log.(pred) .+ (1.0 .- y) .* log.(1.0 .- pred))\n",
    "\n",
    "end\n",
    "\n",
    "get_loss = function(x, θ, y, l2=6e-4)\n",
    "\n",
    "    pred = f(x, θ)\n",
    "    loss = log_loss(y, pred)\n",
    "    loss = loss + l2 * (sum(abs.(θ[:wxh].^2)) + sum(abs.(θ[:why].^2)))\n",
    "    return loss\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `gradient` function from Zygote does as the name suggests. We need to give `gradient` a function that returns a scalar (_i.e._ an objective function in this case), which is why we made an explicit `get_loss` function earlier. We'll store the results in a dictionary called $d\\theta$, and update our model parameters by following gradient descent. We won't be training with stochastic gradient descent, because in this example we're not using minibatches. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr = 1e1;\n",
    "x, y = get_xor(64,5);\n",
    "θ = init_weights(5);\n",
    "\n",
    "old_weights = append!(reshape(θ[:wxh], size(θ[:wxh])[1]*size(θ[:wxh])[2]),\n",
    "    reshape(θ[:why], size(θ[:why])[1] * size(θ[:why])[2]))\n",
    "    \n",
    "    \n",
    "dθ = gradient((θ) -> get_loss(x, θ, y), θ)\n",
    "plt = scatter(old_weights, label = \"old_weights\")\n",
    "\n",
    "θ[:wxh], θ[:why] = θ[:wxh] .- lr .* dθ[1][:wxh], θ[:why] .- lr .* dθ[1][:why]   \n",
    "\n",
    "new_weights = append!(reshape(θ[:wxh], size(θ[:wxh])[1]*size(θ[:wxh])[2]),\n",
    "    reshape(θ[:why], size(θ[:why])[1] * size(θ[:why])[2]))\n",
    "\n",
    "scatter!(new_weights, label=\"new weights\")\n",
    "display(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we need to design the training loop. This function takes training data and parameters as inputs, as well as a few hyperparameters for how long and how fast to train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = function(x, θ, y, max_steps=1000, lr = 1e-2, l2_reg=1e-4)\n",
    "    \n",
    "    disp_every = max_steps // 100\n",
    "\n",
    "    losses = zeros(max_steps)\n",
    "    acc = zeros(max_steps)\n",
    "\n",
    "    for step = 1:max_steps\n",
    "        \n",
    "        pred = f(x, θ)\n",
    "        loss = log_loss(y, pred)\n",
    "        \n",
    "        losses[step] = loss \n",
    "        \n",
    "        acc[step] = get_accuracy(y, pred)\n",
    "\n",
    "        dθ = gradient((θ) -> get_loss(x, θ, y, l2_reg), θ)\n",
    "\n",
    "        θ[:wxh], θ[:why] = θ[:wxh] .- lr .* dθ[1][:wxh], θ[:why] .- lr .* dθ[1][:why]       \n",
    "        \n",
    "        if mod(step, disp_every) == 0\n",
    "            \n",
    "            val_x, val_y = get_xor(512, size(x)[2]);\n",
    "            pred = f(val_x, θ) \n",
    "            loss = log_loss(val_y, pred)\n",
    "            accuracy = get_accuracy(val_y, pred)\n",
    "\n",
    "            println(\"loss at step $step = $loss, accuracy = $accuracy\")\n",
    "            #save_frame(θ, step);\n",
    "\n",
    "        end\n",
    "\n",
    "    end\n",
    "    return θ, losses, acc\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all our functions defined, it's time to set up the data and model and call the training loop. We'll use `violin` plots from the `StatsPlots` package to show how the distributions of weights change over time. Calling a `plot` function with the `!` in-place modifier allows you to add more plots the current figure. If we want to display more than 1 figure per notebook cell, and we do, we need to explicitly call `display` on the figure we want to show. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dim_x = 3\n",
    "dim_h = 4\n",
    "dim_y = 1\n",
    "l2_reg = 1e-4\n",
    "lr = 1e-2\n",
    "max_steps = 1000000\n",
    "\n",
    "θ = init_weights(dim_x, dim_y, dim_h)\n",
    "x, y = get_xor(512, dim_x)\n",
    "\n",
    "println(size(x))\n",
    "\n",
    "\n",
    "plt = violin([\" \"], reshape(θ[:wxh],dim_x * dim_h), label=\"wxh\", title=\"Weights\", alpha = 0.5)\n",
    "violin!([\" \"], reshape(θ[:why],dim_h*dim_y), label=\"why\", alpha = 0.5)\n",
    "display(plt)\n",
    "\n",
    "θ, losses, acc = train(x, θ, y, max_steps, lr, l2_reg)\n",
    "\n",
    "\n",
    "plt = violin([\" \"], reshape(θ[:wxh],dim_x * dim_h), label=\"wxh\", title=\"Weights\", alpha = 0.5)\n",
    "violin!([\" \"], reshape(θ[:why],dim_h*dim_y), label=\"why\", alpha = 0.5)\n",
    "display(plt)\n",
    "\n",
    "steps = 1:size(losses)[1]\n",
    "plt = plot(steps, losses, title=\"Training XOR\", label=\"loss\")\n",
    "plot!(steps, acc, label=\"accuracy\")\n",
    "display(plt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot and gif\n",
    "\n",
    "Here are some plots from a previous training run.  The violin plot gif shows how the weights changed over time and the accuracy/loss plot shows a typical XOR training curve. \n",
    "\n",
    "\n",
    "<img src=\"violin_weights.gif\">\n",
    "<img src=\"temp.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's a good idea to generate a test set to figure how badly our model is overfitted to the training data. If you're unlucky and get poor performance from your model, try changing some of the hyperparameters like learning rate or l2 regularization. You can also generate a larger training dataset for better performance, or try changing the size of the hidden layer by changing `dim_h`. Heck, you could even modify the code to add l1 regularization or add layers to the MLP. Go wild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_x, test_y = get_xor(512,3);\n",
    "\n",
    "pred = f(test_x, θ);\n",
    "test_accuracy = get_accuracy(test_y, pred);\n",
    "test_loss = log_loss(test_y, pred);\n",
    "\n",
    "println(\"Test loss and accuracy are $test_loss and $test_accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing vs validation\n",
    "\n",
    "The difference between a test and validation dataset is immaterial when we're generating our data on demand as we are here. But normally you wouldn't want to go back and modify your training algorithm after running your model on a static test dataset. That sort of behavior runs a high risk of data leakage as you can keep tweaking training until you get good performance, but if stop only when the test score is good you'll actually have settled on a lucky score. That doesn't tell you anything about how the model will behave with actual test data that it hasn't seen before. This happens in the real world when researchers collectively iterate on a few standard dataset. Of course there will be [incremental improvement every year on MNIST](https://arxiv.org/abs/1905.10498) if everyone keeps fitting their research and development strategy to the test set!\n",
    "\n",
    "In any case, thanks for stopping by and I hope you enjoying exploring automatic differentiation in Julia as I did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bonus, functions for plotting a decision surface and saving to disk for a training gif. \n",
    "\n",
    "get_decision_surface = function(θ)\n",
    "    \n",
    "    my_surfacex = []\n",
    "    my_surfacey = []\n",
    "    my_surfacez = []\n",
    "    for xx = -0.25:0.005:1.5-0.25\n",
    "        for yy = -0.25:0.005:1.5-0.25\n",
    "            closest = 50000.\n",
    "            my_coords = [100,100,100]\n",
    "            for zz = -0.25:0.005:1.5-0.25\n",
    "                pred = f(reshape([xx,yy,zz],1,3), θ)\n",
    "                if abs(0.5 - pred[1]) < closest\n",
    "                    my_coords = [xx,yy,zz]\n",
    "                    closest = abs(0.5 - pred[1])\n",
    "                end\n",
    "            end\n",
    "            append!(my_surfacex, my_coords[1])\n",
    "            append!(my_surfacey, my_coords[2])\n",
    "            append!(my_surfacez, my_coords[3])\n",
    "        end\n",
    "    end\n",
    "            \n",
    "    return my_surfacex, my_surfacey, my_surfacez\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "save_frame = function(θ, epoch) \n",
    "\n",
    "    plt = scatter3d([0,0,1,1],[0,1,0,1],[1,0,0,1], \n",
    "        xlim = (-0.05,1.05 ),\n",
    "        ylim = (-0.05,1.05 ),\n",
    "        zlim = (-0.05,1.05 ),\n",
    "        label = \"true\",\n",
    "        markercolor= :blue, markersize=15)\n",
    "\n",
    "    scatter3d!([0,0,1,1],[0,1,0,1],[0,1,1,0], \n",
    "        xlim = (-0.25,1.25 ),\n",
    "        ylim = (-0.25,1.25 ),\n",
    "        zlim = (-0.25,1.25 ),\n",
    "        label = \"false\",\n",
    "        markercolor= :red, markersize=15)\n",
    "\n",
    "\n",
    "    my_surface = get_decision_surface(θ);\n",
    "\n",
    "    scatter3d!(my_surface[1], my_surface[2], my_surface[3],\n",
    "        markercolor = :green,\n",
    "        markerstrokecolor = :green,\n",
    "        markerstrokealpha = 0.0,\n",
    "        markeralpha = 0.05,\n",
    "        label = \"decision boundary\",\n",
    "        legend = :outerbottomright )\n",
    "\n",
    "    for i=1:300 #24\n",
    "        x, y = get_xor(1, 3)\n",
    "\n",
    "        pred = f(x, θ)\n",
    "\n",
    "        if pred[1] > 0.5\n",
    "            my_color = :blue\n",
    "        else\n",
    "            my_color = :red\n",
    "        end\n",
    "\n",
    "        scatter3d!([x[1]], [x[2]], [x[3]], \n",
    "            color = my_color, \n",
    "            alpha = 0.1,\n",
    "            label = \"\")\n",
    "    end \n",
    "\n",
    "    savefig(plt, \"frames/temp$epoch.png\")\n",
    "    \n",
    "    return plt\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
