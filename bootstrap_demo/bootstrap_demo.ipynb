{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bootstrapping Your Way to Uncertainty with Julia Language (Starring Zygote.jl)\n",
    "\n",
    "We know from the [Universal Approximation Theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem) (Hornik 1989 [pdf](https://cognitivemedium.com/magic_paper/assets/Hornik.pdf) and Cybenko 1989 [pdf](https://cognitivemedium.com/magic_paper/assets/Cybenko.pdf)) that neural networks with at least one non-linear hidden layer should be able to represent arbitrary well-behaved functions, and indeed in the last decade we've seen that deep neural networks can handle a wide variety of input-output data mappings. However, neural networks don't intrinsicallyy tell us anything about their confidence: while output activation functions like softmax and sigmoid with a range from 0.0 to 1.0 might _seem_ like they naturally represent probabilities, they don't and treating them like they do can be dangerous. At least with regression models, where the output is some continuously valued scalar or vector of numbers, there isn't the same temptation to equate a prediction in the 0.90s as high-confidence inference. Inference for samples that are out-of-distribution with respect to the training data can be especially misleading. \n",
    "\n",
    "In this essay we'll look at a simple way to generate uncertainty estimates via training several different models on sub-samples of the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\"./bootstrap_demo\")\n",
    "\n",
    "using PyPlot\n",
    "using Gtk\n",
    "using ImageView\n",
    "using Zygote\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These Base functions are explicitly imported so that they can be overloaded w/ multiple dispatch variants\n",
    "\n",
    "import Base.-\n",
    "import Base.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TL;DR Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# uncomment the line below to jump straight into the \n",
    "include(\"./src/bootstrap_demo.jl\")\n",
    "\n",
    "train_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target functions\n",
    "function my_sinc(number_samples::Int64=256, my_range::Array{Float64, 1}=[0.0, 1.0])\n",
    "    \n",
    "    x = (my_range[2] - my_range[1]) .* rand(number_samples, 1) .+ my_range[1]\n",
    "    y = sin.(x .* 10) ./ x\n",
    "    \n",
    "    y = y ./ 10\n",
    "    return x, y\n",
    "    \n",
    "end\n",
    "\n",
    "function my_sinc2(number_samples::Int64=256, my_range::Array{Float64, 1}=[0.0, 1.0])\n",
    "    \n",
    "    x = (my_range[2] - my_range[1]) .* rand(number_samples, 1) .+ my_range[1] \n",
    "  \n",
    "    y = sin.((x.* 10).^2) ./ x\n",
    "     \n",
    "    y = y ./ 10\n",
    "    return x, y\n",
    "    \n",
    "end\n",
    "\n",
    "# target functions\n",
    "\n",
    "function parabola(number_samples::Int64=256, my_range::Array{Float64, 1}=[0.0, 1.0])\n",
    "    \n",
    "    x = (my_range[2] - my_range[1]) .* rand(number_samples, 1) .+ my_range[1] \n",
    "    y =  x.^2\n",
    "    \n",
    "    return x, y\n",
    "    \n",
    "end\n",
    "\n",
    "function y_parabola(number_samples::Int64=256, my_range::Array{Float64, 1}=[0.0, 1.0])\n",
    "    \n",
    "    x = (my_range[2] - my_range[1]) .* rand(number_samples, 1) .+ my_range[1] \n",
    "    \n",
    "    mode = (rand(number_samples) .> 0.5)\n",
    "    println(mode)\n",
    "    \n",
    "    y =  (1.0 .* mode .+ -1.0 .* (1 .- mode)) .*  x.^(1. / 2.) \n",
    "    \n",
    "    return x, y\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and Structs Defining a Simple Multilayer Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structs for MLP\n",
    "\n",
    "mutable struct MLPWeights #<: MultilayerPerceptron\n",
    "   \n",
    "    w_xh::Array{Float64, 2}\n",
    "    w_hh::Array{Float64, 2}\n",
    "    w_hy::Array{Float64, 2} \n",
    "    \n",
    "end\n",
    "\n",
    "mutable struct MLPWeightsAndBiases #<: MultilayerPerceptron\n",
    "   \n",
    "    w_xh::Array{Float64, 2}\n",
    "    w_hh::Array{Float64, 2}\n",
    "    w_hy::Array{Float64, 2}\n",
    "    \n",
    "    bias_xh::Array{Float64, 2}\n",
    "    bias_hh::Array{Float64, 2}\n",
    "    bias_hy::Array{Float64, 2}\n",
    "    \n",
    "end\n",
    "\n",
    "# model initialization\n",
    "\n",
    "function get_mlp(dim_x::Int64, dim_h::Int64, dim_y::Int64, bias::Bool=true)\n",
    "   \n",
    "    w_xh = randn(dim_x, dim_h)\n",
    "    w_hh = randn(dim_h, dim_h)\n",
    "    w_hy = randn(dim_h, dim_y)\n",
    "    \n",
    "    if bias\n",
    "        \n",
    "        bias_xh = randn(1, dim_h)\n",
    "        bias_hh = randn(1, dim_h)\n",
    "        bias_hy = randn(1, dim_y) \n",
    "        mlp = MLPWeightsAndBiases(w_xh, w_hh, w_hy, bias_xh, bias_hh, bias_hy)\n",
    "    else\n",
    "        \n",
    "        mlp = MLPWeights(w_xh, w_hh, w_hy)\n",
    "    end\n",
    "    \n",
    "    return mlp\n",
    "end\n",
    "\n",
    "# model functions (forward and gradient(forward))\n",
    "\n",
    "function forward_mlp(x::Array{Float64,2}, mlp::MLPWeightsAndBiases)\n",
    "    \n",
    "    #act = (x -> tanh.(x))\n",
    "    #act = (x -> (x .* (x .> 0.0) .+ 0.01 .* x))\n",
    "    act = (x -> sin.(x.^2))\n",
    "    \n",
    "    h = act(x * mlp.w_xh .+ mlp.bias_xh)\n",
    "    hh = act(h * mlp.w_hh .+ mlp.bias_hh)\n",
    "    y = hh * mlp.w_hy .+ mlp.bias_hy\n",
    "    \n",
    "    return y\n",
    "    \n",
    "end\n",
    "\n",
    "function forward_mlp(x::Array{Float64,2}, mlp::MLPWeights)\n",
    "    \n",
    "    act = (x -> sin.(x.^2))\n",
    "    #(x -> (x .* (x .> 0.0) .+ 0.01 .* x)) \n",
    "    #(x -> tanh.(x))\n",
    "    \n",
    "    h = act(x * mlp.w_xh)\n",
    "    hh = act(h * mlp.w_hh)\n",
    "    y = hh * mlp.w_hy\n",
    "    \n",
    "    return y\n",
    "    \n",
    "end\n",
    "\n",
    "function get_mlp_loss(x::Array{Float64,2}, y::Array{Float64,2}, mlp::MLPWeightsAndBiases)\n",
    "    # get mse loss for y (target) and prediction (mlp(x))\n",
    "    \n",
    "    pred = forward_mlp(x, mlp)\n",
    "    \n",
    "    loss = sum((y .- pred).^2) / size(pred)[1]\n",
    "    \n",
    "    return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions Facilitating Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function -(mlp::MLPWeightsAndBiases, grad_mlp::Tuple{Base.RefValue{Any}})\n",
    "   \n",
    "    new_mlp = mlp #copy(mlp)\n",
    "    new_mlp.w_xh = mlp.w_xh - grad_mlp[1].x.w_xh\n",
    "    new_mlp.w_hy = mlp.w_hy - grad_mlp[1].x.w_hy\n",
    "    new_mlp.w_hh = mlp.w_hh - grad_mlp[1].x.w_hh\n",
    "    \n",
    "    new_mlp.bias_xh = mlp.bias_xh - grad_mlp[1].x.bias_xh\n",
    "    new_mlp.bias_hh = mlp.bias_hh - grad_mlp[1].x.bias_xh\n",
    "    new_mlp.bias_hy = mlp.bias_hy - grad_mlp[1].x.bias_hy\n",
    "    \n",
    "    return new_mlp\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "function *(lr, mlp::MLPWeightsAndBiases)\n",
    "   \n",
    "    new_mlp = mlp #copy(mlp)\n",
    "    \n",
    "    my_field_names = fieldnames(typeof(mlp))\n",
    "    \n",
    "    for my_field_name in my_field_names\n",
    "       setproperty!(new_mlp, my_field_name, lr .* getproperty(mlp, my_field_name)) \n",
    "        \n",
    "    end\n",
    "    \n",
    "    return new_mlp\n",
    "    \n",
    "end\n",
    "\n",
    "function -(mlp::MLPWeightsAndBiases, grad_mlp::MLPWeightsAndBiases)\n",
    "   \n",
    "    new_mlp = mlp #copy(mlp)\n",
    "    new_mlp.w_xh = mlp.w_xh - grad_mlp.w_xh\n",
    "    new_mlp.w_hy = mlp.w_hy - grad_mlp.w_hy\n",
    "    new_mlp.w_hh = mlp.w_hh - grad_mlp.w_hh\n",
    "    \n",
    "    new_mlp.bias_xh = mlp.bias_xh - grad_mlp.bias_xh\n",
    "    new_mlp.bias_hh = mlp.bias_hh - grad_mlp.bias_hh\n",
    "    new_mlp.bias_hy = mlp.bias_hy - grad_mlp.bias_hy\n",
    "    \n",
    "    return new_mlp\n",
    "    \n",
    "end\n",
    "\n",
    "function get_mlp_gradient(x, y, mlp)\n",
    "    \n",
    "    my_field_names = fieldnames(typeof(mlp))\n",
    "    \n",
    "    grad = Zygote.gradient((mlp) -> get_mlp_loss(x, y, mlp), mlp)\n",
    "    \n",
    "    grad_mlp = typeof(mlp)(0.0 *copy(mlp.w_xh), \n",
    "        0.0 * copy(mlp.w_hy), \n",
    "        0.0 * copy(mlp.w_hh),\n",
    "        0.0 *copy(mlp.bias_xh),\n",
    "        0.0 *copy(mlp.bias_hh), \n",
    "        0.0 *copy(mlp.bias_hy))\n",
    "    \n",
    "    for jj in my_field_names\n",
    "        \n",
    "        #for i in fieldnames(A)\n",
    "        #   setproperty!(a, i, 3)\n",
    "        #en\n",
    "        \n",
    "        setproperty!(grad_mlp, jj, getproperty(grad[1].x, jj))\n",
    "    \n",
    "    end\n",
    "    \n",
    "    return grad_mlp\n",
    "    \n",
    "end\n",
    "\n",
    "function train_mlp(x::Array{Float64,2}, y::Array{Float64,2}, \n",
    "        mlp::MLPWeightsAndBiases, epochs::Int64, lr::Float64=1e-3)\n",
    "\n",
    "    disp_every = epochs / 10\n",
    "    \n",
    "    for epoch = 1:epochs\n",
    "        \n",
    "        if (epoch-1) % disp_every == 0\n",
    "            loss = get_mlp_loss(x, y, mlp)\n",
    "            println(\"loss at epoch $epoch = $loss\") \n",
    "        end\n",
    "        \n",
    "        grad_mlp = get_mlp_gradient(x, y, mlp)\n",
    "        #print(grad_mlp)\n",
    "        mlp = mlp - lr * grad_mlp\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return mlp\n",
    "    \n",
    "end\n",
    "\n",
    "# train a bootstrap ensemble of mlps\n",
    "\n",
    "function train_bootstrap(train_x::Array{Float64,2}, train_y::Array{Float64,2}, model_fn, num_straps::Int)\n",
    "\n",
    "    models = []\n",
    "    number_samples = 1024\n",
    "    \n",
    "    for kk = 1:num_straps\n",
    "        \n",
    "        sample_indices = rand(1:size(train_x)[1], (number_samples,1))\n",
    "        sampled_x = train_x[sample_indices]\n",
    "        sampled_y = train_y[sample_indices]\n",
    "        \n",
    "        mlp = model_fn(1, 64, 1)\n",
    "        \n",
    "        println(\"training model $kk of $num_straps\")\n",
    "        new_mlp = train_mlp(sampled_x, sampled_y, mlp, 5000, 1e-3)\n",
    "        \n",
    "        append!(models, [new_mlp])\n",
    "        \n",
    "    end\n",
    "    \n",
    "    return models\n",
    "    \n",
    "end\n",
    "\n",
    "\n",
    "    \n",
    "function ensemble_prediction(x::Array{Float64,2}, models::Array{Any,1})\n",
    "\n",
    "    prediction = \"\"\n",
    "    \n",
    "    num_models = length(models)\n",
    "    \n",
    "    for model in models\n",
    "       \n",
    "        if prediction == \"\"\n",
    "            prediction = forward_mlp(x, model)\n",
    "        else\n",
    "            prediction = cat(prediction, forward_mlp(x, model), dims=2)\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    \n",
    "\n",
    "    my_mean = (x -> sum(x, dims=2) / size(x)[2] )\n",
    "    #\n",
    "    std_dev = (x -> (sum( (my_mean(x) .- x).^2, dims=2) / size(x)[2]).^0.5)\n",
    "        \n",
    "    mean_pred = my_mean(prediction)\n",
    "    standard_deviation = std_dev(prediction)\n",
    "    \n",
    "    return mean_pred[:,1], standard_deviation[:,1]\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "x, y = my_sinc2(4096, [-.250, .250])\n",
    "\n",
    "models = train_bootstrap(x, y, get_mlp, 13)\n",
    "\n",
    "val_x, val_y = my_sinc2(128, [-.5, .5])\n",
    "\n",
    "val_pred = forward_mlp(val_x, models[1])\n",
    "\n",
    "PyPlot.figure()\n",
    "PyPlot.plot(val_x[:,1], val_y[:,1], \"o\")\n",
    "\n",
    "PyPlot.plot(val_x[:,1], val_pred[:,1], \"x\")\n",
    "PyPlot.title(\"one model\")\n",
    "PyPlot.show()\n",
    "\n",
    "val_pred, val_std = ensemble_prediction(val_x, models)\n",
    "train_pred, train_std = ensemble_prediction(x, models)\n",
    "\n",
    "sort_indices = sortperm(val_x[:,1])\n",
    "\n",
    "val_x = val_x[sort_indices]\n",
    "val_y = val_y[sort_indices]\n",
    "val_pred = val_pred[sort_indices]\n",
    "\n",
    "val_std = val_std[sort_indices]\n",
    "\n",
    "sort_indices = sortperm(x[:,1])\n",
    "x = x[sort_indices]\n",
    "\n",
    "y = y[sort_indices]\n",
    "train_pred = train_pred[sort_indices]\n",
    "train_std = train_std[sort_indices]\n",
    "\n",
    "PyPlot.figure()\n",
    "PyPlot.plot(val_x[:,1], val_y[:,1], \"o\")\n",
    "\n",
    "PyPlot.plot(val_x[:,1], val_pred[:,1], \"x\")\n",
    "PyPlot.fill_between(val_x[:,1], val_pred.-val_std, val_pred.+val_std, alpha=0.5)\n",
    "PyPlot.show()\n",
    "\n",
    "PyPlot.figure()\n",
    "PyPlot.plot(x[:,1], y[:,1], \"o\")\n",
    "\n",
    "PyPlot.plot(x[:,1], train_pred[:,1], \"x\")\n",
    "PyPlot.fill_between(x[:,1], train_pred .- train_std, train_pred .+ train_std, alpha=0.5)\n",
    "PyPlot.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.3",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
