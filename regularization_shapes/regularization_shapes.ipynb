{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Shape of Regularization\n",
    "\n",
    "You know you need to regularize your models to avoid overfitting, but what effects do your choice of regularization method have on a model's parameters? In this tutorial we'll answer that question with a simple experiment comparing parameters in a simple multilayer perceptron (MLP) trained to classify the digits dataset from `scikit-learn` while subject to parameter-value based regularization. The regularization methods we'll specifically focus on are the norm-based method, which you may recognize by their $L_n$ nomeclature. Depending on the $n$, that is, the _order_ of the norm used to regularize a model you may see very different characteristics in the resulting parameter values. Questions regarding regularization effects on neural network models is at least as common as I've personally asked candidates interviewing for data science or machine learning roles (~several), so if you need a financial incentive to continue reading there's that. Of course your life is likely to be much more enjoyable if you are instead driven by the pleasure of figuring things out and building useful tools, but all of that is up to you. Without further ado, let's get into it. \n",
    "\n",
    "Let's start by visualizing and defining norms $L_0, L_1, L_2, L_3, ... L_\\infty$ \n",
    "\n",
    "In general, we can describe the $L_n$ norm as \n",
    "\n",
    "$$\n",
    "\\hspace{6cm} L_n = (\\sum(|x|^n))^\\frac{1}{n} \\hspace{0.2cm}. \\hspace{6cm}(1)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Although we can't raise the sum in **Eq.** 1 by the $\\frac{1}{n}$ power when $n=0$, we can take the limit as $n \\rightarrow \\infty$ to find that the $L_0$ norm is 1.0 for all non-zero scalars, _i.e._ when taking the $L_0$ norm of a vector we'll get the number of non-zero elements in the vector. The animation below visualizes this process.\n",
    "\n",
    "<img src=\"./assets/l0_limit.gif\">\n",
    "\n",
    "As we begin to see for very small values of $n$, the contribution to the $L_0$ norm for any non-zero value is 1 and 0 otherwise.\n",
    "\n",
    "<img src=\"./assets/l0.png\">\n",
    "\n",
    "If we inspect the figure above we see that there's no slope to the $L_0$ norm: it's  totally flat at a value of 1.0 everywhere except 0.0, where there is a discontinuity. That's not very useful for machine learing with gradient-descent, but you can use the $L_0$ norm as a regularization term in algorithms that don't use gradients, such as evolutionary computation, or to compare tensors of parameters to one another. \n",
    "\n",
    "The $L_1$ norm will probably look more familiar and useful. \n",
    "\n",
    "<img src=\"./assets/norm1.png\">\n",
    "\n",
    "Visual inspection of the $L_1$ plot reveals a line with a slope of -1.0 before crossing the y-axis and 1.0 afterward. This is actually a very useful observation! The gradient with respect to the $L_1$ norm of any parameter with a non-zero value will be either 1.0 or -1.0, regardless of the magnitude of said parameter. This means that $L_1$ regularization won't be satisfied until parameter values are 0.0, so any non-zero parameter values better be contributing meaningful functionality to minimizing the overall loss function. In addition to reducing overfitting, $L_1$ regularization encourages sparsity, which can be a desirable characteristic for neural network models for, _e.g._ model compression purposes.\n",
    "\n",
    "Regularizing with higher order norms is markedly different, and this is readily apparent in the plots for $L_2$ and $L_3$ norms. \n",
    "\n",
    "<img src=\"./assets/norm2.png\">\n",
    "\n",
    "Compared to the sharp \"V\" of $L_1$, $L_2$ and $L_3$ demonstrate an increasingly flattened curve around 0.0. As you may intuit from the shape of the curve, this corresponds to low gradient values around x=0. Parameter gradients with respect to these regularization functions are straight lines with a slope equal to the order the norm. Instead of encouraging parameters to take a value of 0.0, norms of higher order will encourage small parameter values. The higher the order, the more emphasis the regularization function puts on penalizing large parameter values. In practice norms with order higher than $L_2$ are very rarely used.\n",
    "\n",
    "<img src=\"./assets/norm3.png\">\n",
    "\n",
    "An interesting thing happens as we approach the $L_\\infty$ norm, typically pronounced as \"L sup\" which is short for \"supremum norm.\" The $L_\\infty$ function returns the maximum absolute parameter value.\n",
    "\n",
    "$$\n",
    "L_\\infty = max(|x|)\n",
    "$$\n",
    "\n",
    "<img src=\"./assets/norm_sup.png\">\n",
    "\n",
    "We can visualize how higher order norms begin to converge toward $L_\\infty$:\n",
    "\n",
    "<img src=\"./assets/ln_norms.gif\">\n",
    "\n",
    "# Example\n",
    "\n",
    "In the previous section we looked at plots for various $L_n$ norms used in machine learning, using our observations to discuss how these regularization methods might affect the parameters of a model during training. Now it's time to see exactly what happens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "my_cmap = plt.get_cmap(\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def norm(x, n, axis=0):\n",
    "    # expects a scalar or vector and returns the nth-order norm for each value\n",
    "    # (i.e. no summing)\n",
    "    # note numpy also has a norm function in the `linalg` module\n",
    "    \n",
    "    my_norm = (np.abs(x)**n)\n",
    "    if n:\n",
    "        my_norm = np.sum(np.abs(x), axis=axis)**n \n",
    "    else:\n",
    "        my_norm[np.abs(x)==np.min(np.abs(x))] = 0.0\n",
    "        my_norm = my_norm[0]\n",
    "        \n",
    "    return my_norm\n",
    "\n",
    "def sup_norm(x):\n",
    "    \n",
    "    x_max = np.max(np.abs(x))\n",
    "    my_norm = np.zeros_like(x)\n",
    "    my_norm[np.abs(x)==x_max] = x_max\n",
    "    return my_norm[0]\n",
    "\n",
    "x = np.linspace(-.20, .20, 128)[np.newaxis,:]\n",
    "\n",
    "l0 = norm(x, 0.0)\n",
    "l1 = norm(x, 1)\n",
    "l2 = norm(x, 2)\n",
    "l3 = norm(x, 3)\n",
    "\n",
    "l_sup = sup_norm(x)\n",
    "\n",
    "\n",
    "my_fontsize=32\n",
    "with plt.xkcd(scale=0.50, length=128.0):\n",
    "    \n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(x[0], l0, \".\", color=my_cmap(0.0), lw=5)\n",
    "    plt.title(\"$L_0$\", fontsize = my_fontsize)\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(x[0], l1, color=my_cmap(0.25), lw=5)\n",
    "    plt.title(\"$L_1$\", fontsize = my_fontsize)\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(x[0], l2, color=my_cmap(0.5), lw=5)\n",
    "    plt.title(\"$L_2$\", fontsize = my_fontsize)\n",
    "  \n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.viridis()\n",
    "    plt.plot(x[0], l3, color=my_cmap(0.75), lw=5)\n",
    "    plt.title(\"$L_3$\", fontsize = my_fontsize)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(x[0], l_sup, color=my_cmap(1.0), lw=5)\n",
    "    plt.title(\"$L_\\infty$\", fontsize = my_fontsize)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize taking the limit as we approach the L0 norm\n",
    "\n",
    "orders = np.linspace(2e-2, 1e-17, 32)\n",
    "x = np.linspace(-1.0,1.0,128)[np.newaxis,:]\n",
    "\n",
    "cc = 0\n",
    "for n in orders:\n",
    "    \n",
    "    with plt.xkcd(scale=0.6, length=128.0):\n",
    "        \n",
    "        fig = plt.figure(figsize=(8,6))\n",
    "        plt.plot(x[0], norm(x, n), color=my_cmap(n/max_norm_order), lw=5)\n",
    "        plt.title(\"$L_n$ for n = {:.2e}\".format(n), fontsize=my_fontsize)\n",
    "        plt.axis((-1.0, 1.0, 0.9, 1.1))\n",
    "        plt.savefig(\"./assets/l0_limit{}.png\".format(cc))\n",
    "        cc += 1\n",
    "        \n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize L0 through L_n norms\n",
    "\n",
    "max_norm_order = 256\n",
    "\n",
    "x = np.linspace(-1.0,1.0,128)[np.newaxis,:]\n",
    "\n",
    "for n in range(max_norm_order):\n",
    "    \n",
    "    with plt.xkcd(scale=0.3, length=128.0):\n",
    "        \n",
    "        fig = plt.figure(figsize=(8,6))\n",
    "        line_type = \".\" if n == 0 else \"-\"\n",
    "        plt.plot(x[0], norm(x, n), line_type, color=my_cmap(n/max_norm_order), lw=5)\n",
    "        plt.title(\"$L${}\".format(n), fontsize=my_fontsize)\n",
    "        plt.savefig(\"./assets/norm{}.png\".format(n))\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets as datasets\n",
    "import autograd\n",
    "from autograd import numpy as np\n",
    "from autograd import grad\n",
    "import numpy.random as npr\n",
    "\n",
    "# load and prep data\n",
    "[x, y] = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "# convert target labels to one-hot encoding\n",
    "y_one_hot = np.zeros((y.shape[0],10))\n",
    "\n",
    "for dd in range(y.shape[0]):\n",
    "    y_one_hot[dd, y[dd]] = 1.0\n",
    "    \n",
    "my_seed = 42\n",
    "\n",
    "npr.seed(my_seed)\n",
    "npr.shuffle(x)\n",
    "\n",
    "npr.seed(my_seed)\n",
    "npr.shuffle(y)\n",
    "\n",
    "# separate training, test and validation data\n",
    "test_size = int(0.2 * x.shape[0])\n",
    "\n",
    "train_x, train_y = x[:-2*test_size], y_one_hot[:-2*test_size]\n",
    "val_x, val_y = x[-2*test_size:-test_size], y_one_hot[-2*test_size:-test_size]\n",
    "test_x, test_y = x[-test_size:], y_one_hot[-test_size:]\n",
    "\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    \n",
    "    x = x - np.max(x)\n",
    "    x = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def cross_entropy(prediction, y):\n",
    "    \n",
    "    return -np.sum( y * np.log(prediction) \\\n",
    "                   + (1-y) * np.log(1-prediction))\n",
    "\n",
    "def forward(x, wx2h, wh2y):\n",
    "    \n",
    "    x = np.matmul(x, wx2h)\n",
    "    # relu\n",
    "    x = np.arctan(x)\n",
    "    \n",
    "    x = np.matmul(x, wh2y)\n",
    "    x = softmax(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def get_loss(x, wx2h, wh2y, y, l_n=[0.,0.,0.,0.], l_sup=0):\n",
    "    \n",
    "    prediction = forward(x, wx2h, wh2y)\n",
    "    \n",
    "    loss = cross_entropy(prediction, y)\n",
    "    \n",
    "    # add regularization\n",
    "    ## l0\n",
    "    loss += l_n[0] * (np.sum(np.sign(np.abs(wx2h)) \\\n",
    "                             + np.sum(np.sign(np.abs(wh2y)))))\n",
    "    ## l1\n",
    "    loss += l_n[1] * (np.sum(np.abs(wx2h)) \\\n",
    "                      + np.sum(np.sign(np.abs(wh2y))))\n",
    "    ## l2\n",
    "    loss += l_n[2] * (np.sum(np.abs(wx2h)**2) \\\n",
    "                      + np.sum(np.sign(np.abs(wh2y)**2)))\n",
    "    ## l3http://localhost:8888/notebooks/regularization_shapes.ipynb#\n",
    "    loss += l_n[3] * (np.sum(np.abs(wx2h)**3) \\\n",
    "                      + np.sum(np.sign(np.abs(wh2y)**3)))\n",
    "    ## l_sup\n",
    "    loss += l_sup * np.max([np.max(np.abs(wx2h)), np.max(np.abs(wh2y))])\n",
    "    \n",
    "    \n",
    "    return loss\n",
    "\n",
    "get_grad = grad(get_loss, argnum=[1,2])\n",
    "\n",
    "def get_accuracy(x, wx2h, wh2y, y):\n",
    "    \n",
    "    prediction = forward(x, wx2h, wh2y)\n",
    "    \n",
    "    accuracy = np.sum(1.*np.argmax(prediction, axis=1) == np.argmax(y, axis=1)) / np.sum(y)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an MLP with 1 hidden layer, no biases\n",
    "dim_x = x.shape[1]\n",
    "dim_y = 10\n",
    "dim_h = 64\n",
    "learning_rate = 1e-3\n",
    "\n",
    "wx2h = npr.randn(dim_x, dim_h) / np.sqrt(dim_x * dim_h)\n",
    "wh2y = npr.randn(dim_h, dim_y) / np.sqrt(dim_h * dim_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 1000\n",
    "disp_every = 100\n",
    "learning_rate = 3e-4\n",
    "\n",
    "for step in range(num_steps):\n",
    "    [dwx2h, dwh2y] = get_grad(train_x, wx2h, wh2y, train_y)\n",
    "\n",
    "    wx2h -= learning_rate * dwx2h\n",
    "    wh2y -= learning_rate * dwh2y\n",
    "    \n",
    "    if step % disp_every == 0.0:\n",
    "    \n",
    "        train_loss = get_loss(train_x, wx2h, wh2y, train_y) / train_x.shape[0]\n",
    "        val_loss = get_loss(val_x, wx2h, wh2y, val_y) / val_x.shape[0]\n",
    "        train_accuracy = get_accuracy(train_x, wx2h, wh2y, train_y)\n",
    "        val_accuracy = get_accuracy(val_x, wx2h, wh2y, val_y)\n",
    "        \n",
    "        print(\"training step {}\\n\".format(step))\n",
    "        print(\"  training loss: {:.3e}\".format(train_loss))\n",
    "        print(\"    training accuracy: {:.3f}\\n\".format(train_accuracy))\n",
    "        print(\"  validation loss: {:.3e}\".format(val_loss))\n",
    "        print(\"    validation accuracy: {:.3f}\".format(val_accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.linspace(-9, 9., 100)\n",
    "plt.plot(xx, np.arctan(xx))\n",
    "plt.plot(xx, np.tanh(xx))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
