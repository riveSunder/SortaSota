{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Shape of Regularization\n",
    "\n",
    "## Introduction\n",
    "\n",
    "You know you need to regularize your models to avoid overfitting, but what effects do your choice of regularization method have on a model's parameters? In this tutorial we'll answer that question with a simple experiment comparing parameters in a simple multilayer perceptron (MLP) trained to classify the digits dataset from `scikit-learn` while subject to parameter-value based regularization. The regularization methods we'll specifically focus on are the norm-based method, which you may recognize by their $L_n$ nomeclature. Depending on the $n$, that is, the _order_ of the norm used to regularize a model you may see very different characteristics in the resulting parameter values. Questions regarding regularization effects on neural network models is at least as common as I've personally asked candidates interviewing for data science or machine learning roles (~several), so if you need a financial incentive to continue reading there's that. Of course your life is likely to be much more enjoyable if you are instead driven by the pleasure of figuring things out and building useful tools, but all of that is up to you. Without further ado, let's get into it. \n",
    "\n",
    "Let's start by visualizing and defining norms $L_0, L_1, L_2, L_3, ... L_\\infty$ \n",
    "\n",
    "In general, we can describe the $L_n$ norm as \n",
    "\n",
    "$$\n",
    "\\hspace{6cm} L_n = (\\sum(|x|^n))^\\frac{1}{n} \\hspace{0.2cm}. \\hspace{6cm}(1)\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Although we can't raise the sum in **Eq.** 1 by the $\\frac{1}{n}$ power when $n=0$, we can take the limit as $n \\rightarrow \\infty$ to find that the $L_0$ norm is 1.0 for all non-zero scalars, _i.e._ when taking the $L_0$ norm of a vector we'll get the number of non-zero elements in the vector. The animation below visualizes this process.\n",
    "\n",
    "<img src=\"./assets/l0_limit.gif\">\n",
    "\n",
    "As we begin to see for very small values of $n$, the contribution to the $L_0$ norm for any non-zero value is 1 and 0 otherwise.\n",
    "\n",
    "<img src=\"./assets/l0.png\">\n",
    "\n",
    "If we inspect the figure above we see that there's no slope to the $L_0$ norm: it's  totally flat at a value of 1.0 everywhere except 0.0, where there is a discontinuity. That's not very useful for machine learing with gradient-descent, but you can use the $L_0$ norm as a regularization term in algorithms that don't use gradients, such as evolutionary computation, or to compare tensors of parameters to one another. \n",
    "\n",
    "The $L_1$ norm will probably look more familiar and useful. \n",
    "\n",
    "<img src=\"./assets/norm1.png\">\n",
    "\n",
    "Visual inspection of the $L_1$ plot reveals a line with a slope of -1.0 before crossing the y-axis and 1.0 afterward. This is actually a very useful observation! The gradient with respect to the $L_1$ norm of any parameter with a non-zero value will be either 1.0 or -1.0, regardless of the magnitude of said parameter. This means that $L_1$ regularization won't be satisfied until parameter values are 0.0, so any non-zero parameter values better be contributing meaningful functionality to minimizing the overall loss function. In addition to reducing overfitting, $L_1$ regularization encourages sparsity, which can be a desirable characteristic for neural network models for, _e.g._ model compression purposes.\n",
    "\n",
    "Regularizing with higher order norms is markedly different, and this is readily apparent in the plots for $L_2$ and $L_3$ norms. \n",
    "\n",
    "<img src=\"./assets/norm2.png\">\n",
    "\n",
    "Compared to the sharp \"V\" of $L_1$, $L_2$ and $L_3$ demonstrate an increasingly flattened curve around 0.0. As you may intuit from the shape of the curve, this corresponds to low gradient values around x=0. Parameter gradients with respect to these regularization functions are straight lines with a slope equal to the order the norm. Instead of encouraging parameters to take a value of 0.0, norms of higher order will encourage small parameter values. The higher the order, the more emphasis the regularization function puts on penalizing large parameter values. In practice norms with order higher than $L_2$ are very rarely used.\n",
    "\n",
    "<img src=\"./assets/norm3.png\">\n",
    "\n",
    "An interesting thing happens as we approach the $L_\\infty$ norm, typically pronounced as \"L sup\" which is short for \"supremum norm.\" The $L_\\infty$ function returns the maximum absolute parameter value.\n",
    "\n",
    "$$\n",
    "L_\\infty = max(|x|)\n",
    "$$\n",
    "\n",
    "<img src=\"./assets/norm_sup.png\">\n",
    "\n",
    "We can visualize how higher order norms begin to converge toward $L_\\infty$:\n",
    "\n",
    "<img src=\"./assets/ln_norms.gif\">\n",
    "\n",
    "## Experiment 1\n",
    "\n",
    "In the previous section we looked at plots for various $L_n$ norms used in machine learning, using our observations to discuss how these regularization methods might affect the parameters of a model during training. Now it's time to see what actually happens when we use different parameter norms for regularization. To this end we'll train a small MLP with one hidden layer on the `scikit-learn` digits dataset. The first question is, and it's a test of the experimental setup as much as anything, does adding $L_n$ regularization reduce overfitting? \n",
    "\n",
    "I wasn't able to get a really dramatic \"swoosh\" example of overfitting as an un-regularized baseline, even with an exxageratingly wide hidden layer with 256 units. None the less, the gap between the training and validation performance with no regularization was 2.9%. \n",
    "\n",
    "```\n",
    "# After training for 50,000 epochs with no regularization, dim_h = 256\n",
    "\n",
    "min/max/mean magnitude and number of zeros:\n",
    "  0.000e+00/1.015e-01/8.837e-03 and 10932.0\n",
    "training step 49999\n",
    "\n",
    "  training loss: 4.720e-01\n",
    "    training accuracy: 0.960\n",
    "\n",
    "  validation loss: 4.503e-01\n",
    "    validation accuracy: 0.931\n",
    "```\n",
    "\n",
    "<img src=\"./assets/progress_lNone.png\">\n",
    "\n",
    "All regularization methods did produce a slightly narrower gap between training and validation performance, but the difference is not so stark as to provide a basis for confident conclusions (though it should guide our thinking when setting up future experiments). A good sanity check for this is that $L_0$ regularization narrowed the training-validation gap by 0.9% in the first experiment, and I have no explanation for why because the regularization has a discontinuity at x=0.0 and is absolutely flat everywhere else. This shouldn't produce any sort of gradient that could regularize model parameters, but there may be numerical approximations used in [`autograd`](https://github.com/HIPS/autograd), which I used as a lightweight automatic differentiation package for these experiments. The same goes for regularization with the $L_\\infty$ norm.\n",
    "\n",
    "It's also worth noting that I did tinker with the experiment for a while and my expectations likely influenced the numbers. As I write this, I haven't touched the test set yet, we'll save that for the very end. \n",
    "\n",
    "The best performance came with $L_3$ regularization, achieving a validation accuracy of 94.1% after training (and 96.0% on the training set), the other techniques had slightly worse performance, but these differences are not large enough to draw much from. As expected, $L_1$ regularization produced parameters with the most 0.0 values, defined as any weight with an absolute value less than 0.001 which we could use as something like a pruning threshold. The differences between the training plots are not really visually discernible, so I've left those out except for $L_1$ and $L_3$ regularization. \n",
    "\n",
    "```\n",
    "# After training for 50,000 epochs with L0 regularization, dim_h = 256\n",
    "min/max/mean magnitude and number of zeros:\n",
    "  0.000e+00/1.146e-01/8.734e-03 and 11071.0\n",
    "training step 49999\n",
    "\n",
    "  training loss: 4.723e-01\n",
    "    training accuracy: 0.961\n",
    "\n",
    "  validation loss: 4.502e-01\n",
    "    validation accuracy: 0.939\n",
    "```\n",
    "\n",
    "\n",
    "<!-- <img src=\"./assets/progress_l0.png\"> -->\n",
    "\n",
    "```\n",
    "# After training for 50,000 epochs with L1 regularization, dim_h = 256\n",
    "\n",
    "min/max/mean magnitude and number of zeros:\n",
    "  0.000e+00/1.252e-01/8.085e-03 and 11782.0\n",
    "training step 49999\n",
    "\n",
    "  training loss: 4.729e-01\n",
    "    training accuracy: 0.958\n",
    "\n",
    "  validation loss: 4.510e-01\n",
    "    validation accuracy: 0.935\n",
    "```\n",
    "\n",
    "<img src=\"./assets/progress_l1.png\"> \n",
    "\n",
    "\n",
    "```\n",
    "# After training for 50,000 epochs with L2 regularization, dim_h = 256\n",
    "\n",
    "min/max/mean magnitude and number of zeros:\n",
    "  0.000e+00/1.093e-01/8.811e-03 and 10980.0\n",
    "training step 49999\n",
    "\n",
    "  training loss: 4.714e-01\n",
    "    training accuracy: 0.958\n",
    "\n",
    "  validation loss: 4.498e-01\n",
    "    validation accuracy: 0.935\n",
    "```\n",
    "\n",
    "\n",
    "<!-- <img src=\"./assets/progress_l2.png\"> -->\n",
    "\n",
    "\n",
    "```\n",
    "# After training for 50,000 epochs with L3 regularization, dim_h = 256\n",
    "\n",
    "min/max/mean magnitude and number of zeros:\n",
    "  0.000e+00/1.102e-01/8.705e-03 and 11093.0\n",
    "training step 49999\n",
    "\n",
    "  training loss: 4.721e-01\n",
    "    training accuracy: 0.960\n",
    "\n",
    "  validation loss: 4.500e-01\n",
    "    validation accuracy: 0.941\n",
    "```\n",
    "\n",
    "<img src=\"./assets/progress_l3.png\">\n",
    "\n",
    "The results of the first experiment were more or less in line with what I expected, but the differences were small enough to warrant a second look. In experiment 2 we'll turn it up to 11 and use an MLP that is way too deep for the number of data size and number of samples in our dataset. This time we'll use an MLP with over a million parameters, about 1000X the number of samples and over 10000X the number of elements in each sample. Overall, the model has about 10X more parameters than the entire `sklean` digits dataset. Statistical learning purists may wan to shake their head vigorously before reading the next part.\n",
    "\n",
    "## Experiment 2: Turning it Up to 11\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Section 1: Introductoy Figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "my_cmap = plt.get_cmap(\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def norm(x, n, axis=0):\n",
    "    # expects a scalar or vector and returns the nth-order norm for each value\n",
    "    # (i.e. no summing)\n",
    "    # note numpy also has a norm function in the `linalg` module\n",
    "    \n",
    "    my_norm = (np.abs(x)**n)\n",
    "    if n:\n",
    "        my_norm = np.sum(np.abs(x), axis=axis)**n \n",
    "    else:\n",
    "        my_norm[np.abs(x)==np.min(np.abs(x))] = 0.0\n",
    "        my_norm = my_norm[0]\n",
    "        \n",
    "    return my_norm\n",
    "\n",
    "def sup_norm(x):\n",
    "    \n",
    "    x_max = np.max(np.abs(x))\n",
    "    my_norm = np.zeros_like(x)\n",
    "    my_norm[np.abs(x)==x_max] = x_max\n",
    "    return my_norm[0]\n",
    "\n",
    "x = np.linspace(-.20, .20, 128)[np.newaxis,:]\n",
    "\n",
    "l0 = norm(x, 0.0)\n",
    "l1 = norm(x, 1)\n",
    "l2 = norm(x, 2)\n",
    "l3 = norm(x, 3)\n",
    "\n",
    "l_sup = sup_norm(x)\n",
    "\n",
    "\n",
    "my_fontsize=32\n",
    "with plt.xkcd(scale=0.50, length=128.0):\n",
    "    \n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(x[0], l0, \".\", color=my_cmap(0.0), lw=5)\n",
    "    plt.title(\"$L_0$\", fontsize = my_fontsize)\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(x[0], l1, color=my_cmap(0.25), lw=5)\n",
    "    plt.title(\"$L_1$\", fontsize = my_fontsize)\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(x[0], l2, color=my_cmap(0.5), lw=5)\n",
    "    plt.title(\"$L_2$\", fontsize = my_fontsize)\n",
    "  \n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.viridis()\n",
    "    plt.plot(x[0], l3, color=my_cmap(0.75), lw=5)\n",
    "    plt.title(\"$L_3$\", fontsize = my_fontsize)\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(x[0], l_sup, color=my_cmap(1.0), lw=5)\n",
    "    plt.title(\"$L_\\infty$\", fontsize = my_fontsize)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize L0 through L_n norms\n",
    "\n",
    "max_norm_order = 256\n",
    "\n",
    "x = np.linspace(-1.0,1.0,128)[np.newaxis,:]\n",
    "\n",
    "for n in range(max_norm_order):\n",
    "    \n",
    "    with plt.xkcd(scale=0.3, length=128.0):\n",
    "        \n",
    "        fig = plt.figure(figsize=(8,6))\n",
    "        line_type = \".\" if n == 0 else \"-\"\n",
    "        plt.plot(x[0], norm(x, n), line_type, color=my_cmap(n/max_norm_order), lw=5)\n",
    "        plt.title(\"$L${}\".format(n), fontsize=my_fontsize)\n",
    "        plt.savefig(\"./assets/norm{}.png\".format(n))\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize taking the limit as we approach the L0 norm\n",
    "\n",
    "orders = np.linspace(2e-2, 1e-17, 32)\n",
    "x = np.linspace(-1.0,1.0,128)[np.newaxis,:]\n",
    "\n",
    "cc = 0\n",
    "for n in orders:\n",
    "    \n",
    "    with plt.xkcd(scale=0.6, length=128.0):\n",
    "        \n",
    "        fig = plt.figure(figsize=(8,6))\n",
    "        plt.plot(x[0], norm(x, n), color=my_cmap(n/max_norm_order), lw=5)\n",
    "        plt.title(\"$L_n$ for n = {:.2e}\".format(n), fontsize=my_fontsize)\n",
    "        plt.axis((-1.0, 1.0, 0.9, 1.1))\n",
    "        plt.savefig(\"./assets/l0_limit{}.png\".format(cc))\n",
    "        cc += 1\n",
    "        \n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Section 2: Experiment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import sklearn.datasets as datasets\n",
    "import autograd\n",
    "from autograd import numpy as np\n",
    "from autograd import grad\n",
    "import numpy.random as npr\n",
    "\n",
    "def softmax(x):\n",
    "    \n",
    "    x = x - np.max(x)\n",
    "    x = np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def cross_entropy(prediction, y):\n",
    "    \n",
    "    return -np.mean( y * np.log(prediction) \\\n",
    "                   + (1-y) * np.log(1-prediction))\n",
    "\n",
    "def forward(x, wx2h, wh2y):\n",
    "    \n",
    "    x = np.matmul(x, wx2h)\n",
    "    # relu\n",
    "    x = np.arctan(x)\n",
    "    \n",
    "    x = np.matmul(x, wh2y)\n",
    "    x = softmax(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def get_loss(x, wx2h, wh2y, y, l_n=[0.,0.,0.,0.], l_sup=0):\n",
    "    \n",
    "    prediction = forward(x, wx2h, wh2y)\n",
    "    \n",
    "    loss = cross_entropy(prediction, y)\n",
    "    \n",
    "    # add regularization\n",
    "    ## l0\n",
    "    loss += l_n[0] * (np.sum(np.sign(np.abs(wx2h)) \\\n",
    "                             + np.sum(np.sign(np.abs(wh2y)))))\n",
    "    ## l1\n",
    "    loss += l_n[1] * (np.sum(np.abs(wx2h)) \\\n",
    "                      + np.sum(np.sign(np.abs(wh2y))))\n",
    "    ## l2\n",
    "    loss += l_n[2] * (np.sum(np.abs(wx2h)**2) \\\n",
    "                      + np.sum(np.sign(np.abs(wh2y)**2)))\n",
    "    ## l3http://localhost:8888/notebooks/regularization_shapes.ipynb#\n",
    "    loss += l_n[3] * (np.sum(np.abs(wx2h)**3) \\\n",
    "                      + np.sum(np.sign(np.abs(wh2y)**3)))\n",
    "    ## l_sup\n",
    "    loss += l_sup * np.max([np.max(np.abs(wx2h)), np.max(np.abs(wh2y))])\n",
    "    \n",
    "    \n",
    "    return loss\n",
    "\n",
    "get_grad = grad(get_loss, argnum=[1,2])\n",
    "\n",
    "def get_accuracy(x, wx2h, wh2y, y):\n",
    "    \n",
    "    prediction = forward(x, wx2h, wh2y)\n",
    "    \n",
    "    accuracy = np.sum(1.*np.argmax(prediction, axis=1) == np.argmax(y, axis=1)) / np.sum(y)\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prep data\n",
    "my_seed = 1337\n",
    "\n",
    "[x, y] = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "npr.seed(my_seed)\n",
    "npr.shuffle(x)\n",
    "\n",
    "npr.seed(my_seed)\n",
    "npr.shuffle(y)\n",
    "\n",
    "# convert target labels to one-hot encoding\n",
    "y_one_hot = np.zeros((y.shape[0],10))\n",
    "\n",
    "for dd in range(y.shape[0]):\n",
    "    y_one_hot[dd, y[dd]] = 1.0\n",
    "    \n",
    "# separate training, test and validation data\n",
    "test_size = int(0.3 * x.shape[0])\n",
    "\n",
    "train_x, train_y = x[:-2*test_size], y_one_hot[:-2*test_size]\n",
    "val_x, val_y = x[-2*test_size:-test_size], y_one_hot[-2*test_size:-test_size]\n",
    "test_x, test_y = x[-test_size:], y_one_hot[-test_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define an MLP with 1 hidden layer, no biases\n",
    "dim_x = x.shape[1]\n",
    "dim_y = 10\n",
    "dim_h = 64\n",
    "learning_rate = 1e-3\n",
    "\n",
    "def get_weights(dim_x=64, dim_h=4, dim_y=10):\n",
    "    \n",
    "    wx2h = npr.randn(dim_x, dim_h) / np.sqrt(dim_x * dim_h)\n",
    "    wh2y = npr.randn(dim_h, dim_y) / np.sqrt(dim_h * dim_y)\n",
    "    \n",
    "    return wx2h, wh2y\n",
    "\n",
    "wx2h, wh2y = get_weights()\n",
    "\n",
    "restore_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_steps = 50000\n",
    "disp_every = 500\n",
    "learning_rate = 3e-4\n",
    "\n",
    "if restore_weights:\n",
    "    weight_index = 0\n",
    "else:\n",
    "    list_wx2h = []\n",
    "    list_wh2y = []\n",
    "    \n",
    "for ln in [[0,0,0,0.], [1e-4,0.,0.,0.], [0.,1e-4,0.,0.],\\\n",
    "          [0.,0.,1e-4,0.], [0.,0.,0.,1e-4]]:\n",
    "    \n",
    "    if restore_weights:\n",
    "        wx2h = list_wx2h[weight_index]\n",
    "        wh2y = list_wh2y[weight_index]\n",
    "        weight_index += 1\n",
    "    else:\n",
    "        wx2h, wh2y = get_weights(dim_h=256)\n",
    "        \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    epochs = []\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        [dwx2h, dwh2y] = get_grad(train_x, wx2h, wh2y, train_y, l_n=ln)\n",
    "\n",
    "        wx2h -= learning_rate * dwx2h\n",
    "        wh2y -= learning_rate * dwh2y\n",
    "\n",
    "        if step % disp_every == 0.0:\n",
    "            train_pred = forward(train_x, wx2h, wh2y)\n",
    "            train_loss = cross_entropy(train_pred, train_y)\n",
    "            \n",
    "            val_pred = forward(val_x, wx2h, wh2y)\n",
    "            val_loss = cross_entropy(val_pred, val_y)\n",
    "            \n",
    "            train_accuracy = get_accuracy(train_x, wx2h, wh2y, train_y)\n",
    "            val_accuracy = get_accuracy(val_x, wx2h, wh2y, val_y)\n",
    "                        \n",
    "            if(0):\n",
    "                train_loss = get_loss(train_x, wx2h, wh2y, train_y) #/ train_x.shape[0]\n",
    "                val_loss = get_loss(val_x, wx2h, wh2y, val_y) #/ val_x.shape[0]\n",
    "                \n",
    "            if(0): # reach unreachable code below for progress reports \n",
    "                print(\"training step {}\\n\".format(step))\n",
    "                print(\"  training loss: {:.3e}\".format(train_loss))\n",
    "                print(\"    training accuracy: {:.3f}\\n\".format(train_accuracy))\n",
    "                print(\"  validation loss: {:.3e}\".format(val_loss))\n",
    "                print(\"    validation accuracy: {:.3f}\".format(val_accuracy))\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            epochs.append(step)\n",
    "    \n",
    "    list_wx2h.append(wx2h)\n",
    "    list_wh2y.append(wh2y)\n",
    "    \n",
    "    l_type = np.argmax(ln) if np.sum(ln) else \"(None)\"\n",
    "    \n",
    "    all_params = np.abs(np.append(wx2h.ravel(), wh2y.ravel()))\n",
    "    all_params[all_params < 1e-2] = 0.0\n",
    "    my_mean = np.mean(all_params)\n",
    "    my_min = np.min(all_params)\n",
    "    my_max = np.max(all_params)\n",
    "    \n",
    "    num_zeros = all_params.shape[0] - np.sum(np.sign(np.abs(all_params)))\n",
    "    \n",
    "    print(\"min/max/mean magnitude and number of zeros:\")\n",
    "    print(\"  {:.3e}/{:.3e}/{:.3e} and {}\"\\\n",
    "          .format(my_min, my_max, my_mean, num_zeros))\n",
    "    \n",
    "    print(\"training step {}\\n\".format(step))\n",
    "    print(\"  training loss: {:.3e}\".format(train_loss))\n",
    "    print(\"    training accuracy: {:.3f}\\n\".format(train_accuracy))\n",
    "    print(\"  validation loss: {:.3e}\".format(val_loss))\n",
    "    print(\"    validation accuracy: {:.3f}\".format(val_accuracy))\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(wx2h, \\\n",
    "             vmin=-.20, vmax=.20)\n",
    "    plt.title(\"wx2h for L{}\".format(l_type), fontsize=32)\n",
    "    plt.colorbar()\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(wh2y, \\\n",
    "             vmin=-.20, vmax=.20)\n",
    "    plt.title(\"wh2y for L{}\".format(l_type), fontsize=32)\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"./assets/weights_L{}.png\".format(l_type))\n",
    "    \n",
    "    \n",
    "    with plt.xkcd(scale=0.3, length=128):\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(epochs, train_losses, color=my_cmap(0.0), lw=5, label=\"training loss\")\n",
    "        plt.plot(epochs, val_losses, color=my_cmap(0.5), lw=5, label=\"validation loss\")\n",
    "        plt.axis((0, num_steps, 0.2, 0.80))\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.title(\"losses for L{}\".format(l_type), fontsize=32)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        plt.plot(epochs, train_accuracies, color=my_cmap(0.75), lw=5, label=\"training acc.\")\n",
    "        plt.plot(epochs, val_accuracies, color=my_cmap(1.0), lw=5, label=\"validation acc.\")\n",
    "        plt.axis((0, num_steps, 0.0, 1.0))\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.legend()\n",
    "        plt.title(\"L{} accuracy\".format(l_type), fontsize=32)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./assets/progress_l{}.png\".format(l_type))\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "restore_weights = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Section 3: Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefine new functions for get_weights, forward and get_loss to define a \"deep\" MLP* and include l_sup regularization in the main reg. list\n",
    "#\n",
    "# * As per an offhand quip from Geoffrey Hinton in his previously available Coursera course on neural networks,\\\n",
    "# deep learning begins at 7 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_deep_weights(dim_x=64, dim_h=4, dim_y=10, depth=7):\n",
    "    \n",
    "    weights = []\n",
    "    weights.append(npr.randn(dim_x, dim_h) / np.sqrt(dim_x * dim_h))\n",
    "    \n",
    "    for ww in range(1,depth-1):\n",
    "        weights.append(npr.randn(dim_h, dim_h) / np.sqrt(dim_h**2))\n",
    "    \n",
    "    weights.append(npr.randn(dim_h, dim_y))\n",
    "        \n",
    "    return weights\n",
    "\n",
    "def deep_forward(x, weights, dropout=0.0):\n",
    "    \n",
    "    for dd in range(len(weights)):\n",
    "        if dd:\n",
    "            x = np.arctan(x)\n",
    "        \n",
    "        x = np.matmul(x, weights[dd])\n",
    "        if dropout:\n",
    "            dropout_map = np.random.random((x.shape[0], x.shape[1]))\n",
    "            x[dropout_map < dropout] *= 0.0\n",
    "            \n",
    "            x /= (1 - dropout)\n",
    "        \n",
    "    x = softmax(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def get_all_params(weights):\n",
    "    \n",
    "    all_params = np.append(weights[0].ravel(), weights[1].ravel())\n",
    "    \n",
    "    for ww in range(1, len(weights)):\n",
    "        all_params = np.append(all_params, weights[ww].ravel())\n",
    "    \n",
    "    return all_params\n",
    "\n",
    "def get_deep_loss(x, weights, y, l_n=[0.,0.,0.,0.,0.], dropout=0.0):\n",
    "    \n",
    "    prediction = deep_forward(x, weights, dropout)\n",
    "    \n",
    "    loss = cross_entropy(prediction, y)\n",
    "    \n",
    "    # add regularization\n",
    "    all_params = get_all_params(weights) \n",
    "    ## l0\n",
    "    loss += l_n[0] * (np.mean(np.sign(np.abs(all_params))))\n",
    "    ## l1\n",
    "    loss += l_n[1] * (np.mean(np.abs(all_params)))\n",
    "    ## l2\n",
    "    loss += l_n[2] * (np.mean(np.abs(all_params)**2))\n",
    "    \n",
    "    ## l3\n",
    "    loss += l_n[3] * (np.mean(np.abs(all_params)**3))\n",
    "    ## l_sup\n",
    "    loss += l_n[-1] * np.max(np.abs(all_params))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def get_deep_accuracy(x, weights, y):\n",
    "    \n",
    "    prediction = deep_forward(x, weights)\n",
    "    \n",
    "    accuracy = np.sum(1.*np.argmax(prediction, axis=1) == np.argmax(y, axis=1)) / np.sum(y)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "get_deep_grad = grad(get_deep_loss, argnum=1)\n",
    "restore_weights = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_steps = 50000\n",
    "disp_every = 50\n",
    "learning_rate = 3e-2\n",
    "reg_scale = 1e-1\n",
    "\n",
    "if restore_weights:\n",
    "    weight_index = 0\n",
    "else:\n",
    "    list_weights = []\n",
    "    \n",
    "for ln in [[0,0,0,0.,0.], [reg_scale,0.,0.,0.,0.], [0.,reg_scale,0.,0.,0.],\\\n",
    "          [0.,0.,reg_scale,0.,0.], [0.,0.,0.,reg_scale,0.], [0.,0.,0.,0.,reg_scale], None]:\n",
    "    \n",
    "    if ln is None:\n",
    "        ln = [0.,0.,0.,0.,0.]\n",
    "        dropout_rate = 0.25\n",
    "    else:\n",
    "        dropout_rate = 0.0\n",
    "        \n",
    "        \n",
    "    weights = get_deep_weights(dim_h=16)\n",
    "        \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    epochs = []\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "        \n",
    "        if step % disp_every == 0.0:\n",
    "            train_pred = deep_forward(train_x, weights)\n",
    "            train_loss = cross_entropy(train_pred, train_y)\n",
    "            \n",
    "            val_pred = deep_forward(val_x, weights)\n",
    "            val_loss = cross_entropy(val_pred, val_y)\n",
    "            \n",
    "            train_accuracy = get_deep_accuracy(train_x, weights, train_y)\n",
    "            val_accuracy = get_deep_accuracy(val_x, weights, val_y)\n",
    "                          \n",
    "            if(0): # reach unreachable code below for progress reports \n",
    "                print(\"training step {}\\n\".format(step))\n",
    "                print(\"  training loss: {:.3e}\".format(train_loss))\n",
    "                print(\"    training accuracy: {:.3f}\\n\".format(train_accuracy))\n",
    "                print(\"  validation loss: {:.3e}\".format(val_loss))\n",
    "                print(\"    validation accuracy: {:.3f}\".format(val_accuracy))\n",
    "                \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            epochs.append(step)\n",
    "            \n",
    "        grads = get_deep_grad(train_x, weights, train_y, l_n=ln)\n",
    "\n",
    "        for ii in range(len(weights)):\n",
    "            weights[ii] -= learning_rate * grads[ii]\n",
    "            \n",
    "\n",
    "    \n",
    "    list_weights.append(weights)\n",
    "    \n",
    "    l_type = \"L\" + str(np.argmax(ln)) if np.sum(ln) else \"noreg\"\n",
    "    if l_type == 4:\n",
    "        l_type = \"sup\"\n",
    "    elif dropout_rate:\n",
    "        l_type = \"dropout\"\n",
    "    \n",
    "    all_params = get_all_params(weights) \n",
    "    \n",
    "    all_params[all_params < 1e-3] = 0.0\n",
    "    my_mean = np.mean(all_params)\n",
    "    my_min = np.min(all_params)\n",
    "    my_max = np.max(all_params)\n",
    "    \n",
    "    num_zeros = all_params.shape[0] - np.sum(np.sign(np.abs(all_params)))\n",
    "    \n",
    "    print(\"min/max/mean magnitude and number of zeros:\")\n",
    "    print(\"  {:.3e}/{:.3e}/{:.3e} and {}\"\\\n",
    "          .format(my_min, my_max, my_mean, num_zeros))\n",
    "    \n",
    "    print(\"training step {}\\n\".format(step))\n",
    "    print(\"  training loss: {:.3e}\".format(train_loss))\n",
    "    print(\"    training accuracy: {:.3f}\\n\".format(train_accuracy))\n",
    "    print(\"  validation loss: {:.3e}\".format(val_loss))\n",
    "    print(\"    validation accuracy: {:.3f}\".format(val_accuracy))\n",
    "\n",
    "        \n",
    "    with plt.xkcd(scale=0.3, length=128):\n",
    "        plt.figure(figsize=(12,6))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(epochs, train_losses, color=my_cmap(0.0), lw=5, label=\"training loss\")\n",
    "        plt.plot(epochs, val_losses, color=my_cmap(0.5), lw=5, label=\"validation loss\")\n",
    "        plt.axis((0, num_steps, 0.2, 0.80))\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.title(\"losses with {}\".format(l_type), fontsize=32)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(122)\n",
    "        plt.plot(epochs, train_accuracies, color=my_cmap(0.75), lw=5, label=\"training acc.\")\n",
    "        plt.plot(epochs, val_accuracies, color=my_cmap(1.0), lw=5, label=\"validation acc.\")\n",
    "        plt.axis((0, num_steps, 0.0, 1.0))\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"accuracy\")\n",
    "        plt.legend()\n",
    "        plt.title(\"accuracy with {}\".format(l_type), fontsize=32)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"./assets/exp2_progress_{}.png\".format(l_type))\n",
    "        plt.show()\n",
    "        \n",
    "\n",
    "restore_weights = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
