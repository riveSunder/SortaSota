{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autograd import numpy as np\n",
    "from autograd import grad\n",
    "import numpy.random as npr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_loss(y_tgt, pred):\n",
    "    \n",
    "    num_classes = y_tgt.shape[1]\n",
    "    N = y_tgt.shape[0]\n",
    "    \n",
    "    return -1/N * np.sum( y_tgt * np.log(pred[:,:num_classes]) \\\n",
    "                      + (1. - y_tgt) * np.log(1. - pred[:,:num_classes]) )\n",
    "\n",
    "def mse_loss(y_tgt, pred):\n",
    "    \n",
    "    return np.mean((y_tgt - pred)**2)\n",
    "\n",
    "\n",
    "def mae_loss(y_tgt, pred):\n",
    "    \n",
    "    return np.mean(np.abs(y_tgt - pred))\n",
    "    \n",
    "def get_log_loss(nn_fn, x, weights, acts, y_tgt):\n",
    "    \n",
    "    pred = nn_fn(x, weights, acts)\n",
    "    \n",
    "    loss = log_loss(y_tgt, pred) \n",
    "    \n",
    "    return loss\n",
    "\n",
    "    \n",
    "def get_mse_loss(nn_fn, x, weights, acts, y_tgt):\n",
    "    \n",
    "    pred = nn_fn(x, weights, acts)\n",
    "    \n",
    "    loss = mse_loss(y_tgt, pred) \n",
    "    \n",
    "    return loss\n",
    "\n",
    "def get_accuracy(y, pred):\n",
    "    \n",
    "    num_classes = y.shape[1]\n",
    "    \n",
    "    temp = [1.0 * (elem1 == elem2) \\\n",
    "            for elem1, elem2 in zip(np.argmax(y,axis=1), np.argmax(pred[:,:num_classes],axis=1))]\n",
    "    \n",
    "    return np.mean(temp) \n",
    "\n",
    "gradloss = grad(get_log_loss)\n",
    "grad_mse_loss = grad(get_mse_loss)\n",
    "\n",
    "def nn_pass(x, weights, acts, forward=True):\n",
    "    \n",
    "    assert len(weights) == len(acts)\n",
    "\n",
    "    \n",
    "    #if not(forward):\n",
    "    #    acts.reverse()\n",
    "    \n",
    "    output_mask = np.zeros((1, weights[-1].shape[1]))\n",
    "    output_mask[:, :10] += 1.0\n",
    "    \n",
    "    for w, a in zip(weights, acts):\n",
    "        \n",
    "        x = a(np.matmul(x,w))\n",
    "    \n",
    " \n",
    "    x = x * output_mask    \n",
    "    x = acts[-1](x)\n",
    "    return x       \n",
    "        \n",
    "\n",
    "def bnn_pass(x, weights, act, forward=True, regression=True):\n",
    "    \n",
    "    if forward:\n",
    "        c = 1.0\n",
    "    else:\n",
    "        c = -1.0\n",
    "    \n",
    "    dim_x = x.shape[1]   \n",
    "    output_mask = np.zeros((1, 2*weights[-1].shape[1]))\n",
    "    output_mask[:, :10] += 1.0\n",
    "    \n",
    "    halfsies = dim_x // 2\n",
    "    \n",
    "    flips = [1]\n",
    "    \n",
    "    for ii in range(1, len(weights)):\n",
    "        flips.append(1.0 - flips[-1])\n",
    "        \n",
    "    v1 = x[:,:halfsies]\n",
    "    v2 = x[:,halfsies:]\n",
    "    \n",
    "    assert len(weights) == len(act)\n",
    "    \n",
    "    if not(forward):\n",
    "        acts.reverse()\n",
    "    \n",
    "     \n",
    "    for ii in range(len(weights)): #w, a, flip  in zip(weights, acts, flips):\n",
    "        \n",
    "        v1 = v1\n",
    "         \n",
    "        v2 = v2 + c * acts[ii](np.matmul(v1, weights[ii]))\n",
    "        \n",
    "        v = np.append(v1, v2, axis=1)\n",
    "        \n",
    "        if flips[ii]:\n",
    "            v1 = v[:,:halfsies]\n",
    "            v2 = v[:,halfsies:]\n",
    "        else:\n",
    "            v1 = v[:,halfsies:]\n",
    "            v2 = v[:,:halfsies]\n",
    "    \n",
    "    \n",
    "    #final layer is not bijective\n",
    "    #v = acts[-1](np.matmul(v, weights[-1]))\n",
    "    if not regression:\n",
    "        v = v * output_mask    \n",
    "        v = acts[-1](v)\n",
    "    \n",
    "    return v\n",
    "\n",
    "def softmax(x):\n",
    "    \n",
    "    x = x - np.max(x, axis=1, keepdims=True) + 1.e-5\n",
    "    \n",
    "    return np.exp(x)  / np.sum(np.exp(x), axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def get_one_hot(tgt_y):\n",
    "    \n",
    "    num_samples, num_classes = np.shape(tgt_y)[0], np.max(1+tgt_y)\n",
    "    \n",
    "    one_hot_y = np.array([[1.0 if jj == tgt_y[ii] else 0.0 for jj in range(num_classes)] \\\n",
    "                      for ii in range(num_samples)])\n",
    "    return one_hot_y \n",
    "    \n",
    "def leaky_relu(x):\n",
    "    \n",
    "    #x[x <= 0.0] = 0.0 # -0.02* x[x<0]\n",
    "    \n",
    "    return x**2+ 1e-3\n",
    "    \n",
    "\n",
    "def adam_update(l_grad, l_m=None, l_v=None):\n",
    "    # n = running exponential average of first moment of gradient\n",
    "    # v = running exponential average of second moment of gradient\n",
    "    # grad = gradient of current batch\n",
    "    \n",
    "    β1 = 0.9\n",
    "    β2 = 0.999\n",
    "    ϵ = 1e-7\n",
    "    \n",
    "    l_update = []\n",
    "    l_mt1 = []\n",
    "    l_vt1 = []\n",
    "    \n",
    "    if l_m is None:\n",
    "        l_m = [elem * 0.0 for elem in l_grad]\n",
    "    if l_v is None:\n",
    "        l_v = [elem * 0.0 for elem in l_grad]\n",
    "            \n",
    "    for my_grad, m, v in zip(l_grad, l_m, l_v):\n",
    "    \n",
    "        m = β1 * m + (1-β1) * my_grad\n",
    "        v = β2 * v + (1-β2) * my_grad**2\n",
    "    \n",
    "        l_update.append((m / (np.sqrt(v) + ϵ))) \n",
    "        l_mt1.append(m)\n",
    "        l_vt1.append(v)\n",
    "    \n",
    "    return l_update, l_mt1, l_vt1\n",
    "\n",
    "def relu(x):\n",
    "    return x * (x > 0.0)\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return x * (x > 0.0) + 0.1 * x * (x <= 0.0)\n",
    "\n",
    "def prelu(x,a=0.01):\n",
    "    return x * (x > 0.0) + a * x * (x <= 0.0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def swish(x):\n",
    "    return x * sigmoid(x)\n",
    "\n",
    "def elu(x, a = 0.1): \n",
    "    return x * (x > 0.0) + a * (np.exp(x)-1) * (x <= 0.0)\n",
    "    \n",
    "\n",
    "def sinc(x):\n",
    "\n",
    "    return np.sin(16*x) / (16*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def bnn_pass(x, weights, act, forward=True, regression=True):\n",
    "    \n",
    "    if forward:\n",
    "        c = 1.0\n",
    "    else:\n",
    "        c = -1.0\n",
    "    \n",
    "    dim_x = x.shape[1]   \n",
    "    output_mask = np.zeros((1, 2*weights[-1].shape[1]))\n",
    "    output_mask[:, :10] += 1.0\n",
    "    \n",
    "    halfsies = dim_x // 2\n",
    "    \n",
    "    flips = [1]\n",
    "    \n",
    "    for ii in range(1, len(weights)):\n",
    "        flips.append(1.0 - flips[-1])\n",
    "        \n",
    "    v1 = x[:,:halfsies]\n",
    "    v2 = x[:,halfsies:]\n",
    "    \n",
    "    assert len(weights) == len(act)\n",
    "    \n",
    "    if not(forward):\n",
    "        acts.reverse()\n",
    "    \n",
    "     \n",
    "    for ii in range(len(weights)): #w, a, flip  in zip(weights, acts, flips):\n",
    "        \n",
    "        v1 = v1\n",
    "         \n",
    "        v2 = v2 + c * acts[ii](np.matmul(v1, weights[ii]))\n",
    "        \n",
    "        v = np.append(v1, v2, axis=1)\n",
    "        \n",
    "        if flips[ii]:\n",
    "            v1 = v[:,:halfsies]\n",
    "            v2 = v[:,halfsies:]\n",
    "        else:\n",
    "            v1 = v[:,halfsies:]\n",
    "            v2 = v[:,:halfsies]\n",
    "    \n",
    "    \n",
    "    #final layer is not bijective\n",
    "    #v = acts[-1](np.matmul(v, weights[-1]))\n",
    "    if not regression:\n",
    "        v = v * output_mask    \n",
    "        v = acts[-1](v)\n",
    "    \n",
    "    return v\n",
    "\n",
    "\n",
    "wbnn = []\n",
    "acts = []\n",
    "\n",
    "x = np.random.random((1,64))\n",
    "\n",
    "for layers in range(6):\n",
    "    wbnn.append(npr.randn(32,32))\n",
    "    acts.append(np.tanh)\n",
    "    pred = bnn_pass(x, wbnn, acts, forward=True)\n",
    "    rpred = bnn_pass(pred, wbnn, acts, forward=False)\n",
    "\n",
    "   \n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(x[0]-rpred[0])\n",
    "    plt.title(\"{} layers input - rpred\".format(1+layers))\n",
    "    plt.show()\n",
    "    \n",
    "    if(1):\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.plot(x[0]-pred[0])\n",
    "        plt.title(\"{} layers input - output\".format(1+layers))\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,64).reshape(1,64)\n",
    "y = sinc(x)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x[0],y[0])\n",
    "plt.show()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-1,1,64).reshape(1,64)\n",
    "y = sinc(2*x)\n",
    "\n",
    "wbnn = [npr.randn(32,32)/32**2, npr.randn(32,32)/(32*32)]#, npr.randn(32,32)/(32*32)]\n",
    "acts = [relu, relu]#, np.tanh]\n",
    "\n",
    "loss = get_mse_loss(bnn_pass, y, wbnn, acts, y)\n",
    "print(\"loss = {:.2f}\".format(epoch, loss))\n",
    "\n",
    "pred = bnn_pass(y, wbnn, acts)\n",
    "predr = bnn_pass(pred, wbnn, acts, forward=False)\n",
    "plt.figure()\n",
    "plt.plot(x[0], y[0], label=\"target\")\n",
    "plt.plot(x[0], pred[0], label=\"forward pred.\")\n",
    "plt.plot(x[0], predr[0], label=\"reverse pred.\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "grad_mse_loss = grad(get_mse_loss, argnum=2)\n",
    "m, v = None, None\n",
    "\n",
    "lr = 3e-9\n",
    "l2_reg = 1e-3\n",
    "l1_reg = 1e-3\n",
    "\n",
    "for epoch in range(10):\n",
    "    if epoch % 1 == 0:\n",
    "        lr = lr * 0.9\n",
    "        pred = bnn_pass(y, wbnn, acts)\n",
    "        loss = mse_loss(y, pred)\n",
    "\n",
    "        print(\"step {} loss = {:.2f}\".format(epoch, loss))\n",
    "        \n",
    "        pred = bnn_pass(x, wbnn, acts)\n",
    "        loss = get_mse_loss(bnn_pass, y, wbnn, acts, y)\n",
    "        \n",
    "        print(\"step {} validation loss = {:.2f}\".format(epoch, loss) )\n",
    "        print(lr)\n",
    "    \n",
    "    grads = grad_mse_loss(bnn_pass, y, wbnn, acts, y)\n",
    "    \n",
    "    update, m, v = adam_update(grads, m, v)\n",
    "    \n",
    "    if epoch > 500:\n",
    "    \n",
    "        l2_reg = 1e-9\n",
    "        l1_reg = 1e-9\n",
    "    \n",
    "    for my_grad, my_params in zip(update, wbnn):\n",
    "\n",
    "        my_params -= lr * (my_grad + l1_reg * np.abs(my_params) + l2_reg * np.abs(my_params)**2)\n",
    "\n",
    "pred = bnn_pass(y, wbnn, acts)\n",
    "predr = bnn_pass(pred, wbnn, acts, forward=False)\n",
    "plt.figure()\n",
    "plt.plot(x[0], y[0], label=\"target\")\n",
    "plt.plot(x[0], pred[0], label=\"forward pred.\")\n",
    "plt.plot(x[0], predr[0], label=\"reverse pred.\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x[0], y[0])\n",
    "plt.plot(x[0], pred[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_grad = [npr.randn(32,32)] * 3\n",
    "\n",
    "g, m, v = adam_update(my_grad, m, v)\n",
    "\n",
    "plt.figure()\n",
    "plt.subplot(131)\n",
    "plt.imshow(g[1])\n",
    "plt.subplot(132)\n",
    "plt.imshow(m[1])\n",
    "plt.subplot(133)\n",
    "plt.imshow(v[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = datasets.load_digits(return_X_y=True)\n",
    "\n",
    "x /= 255.\n",
    "y = get_one_hot(y)\n",
    "\n",
    "my_seed = 13\n",
    "npr.seed(my_seed)\n",
    "npr.shuffle(x)\n",
    "my_seed = 13\n",
    "npr.seed(my_seed)\n",
    "npr.shuffle(y)\n",
    "\n",
    "val_x, val_y = x[:128,:], y[:128]\n",
    "\n",
    "test_x, test_y = x[128:256,:], y[128:256]\n",
    "\n",
    "train_x, train_y = x[256:,:], y[256:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for epoch in range(100):\n",
    "lr = 3e-2\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    return x * (x > 0.0)\n",
    "\n",
    "def leaky_relu(x):\n",
    "    return x * (x > 0.0) + 0.1 * x * (x <= 0.0)\n",
    "\n",
    "def prelu(x,a=0.01):\n",
    "    return x * (x > 0.0) + a * x * (x <= 0.0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def swish(x):\n",
    "    return x * sigmoid(x)\n",
    "\n",
    "def elu(x, a = 0.1): \n",
    "    return x * (x > 0.0) + a * (np.exp(x)-1) * (x <= 0.0)\n",
    "    \n",
    "#wbnn = [npr.randn(32,32)/32**2, npr.randn(32,32)/32**2, npr.randn(32,32)/32**2, npr.randn(32,32)/(32*32)]\n",
    "acts = [relu, elu, swish,  softmax]\n",
    "\n",
    "gradloss = grad(get_log_loss, argnum=2)\n",
    "m, v = None, None\n",
    "\n",
    "\n",
    "l2_reg = 1e-3\n",
    "l1_reg = 1e-3\n",
    "\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    if epoch % 50 == 0:\n",
    "        lr = lr * 0.9\n",
    "        pred = bnn_pass(train_x, wbnn, acts)\n",
    "        loss = get_log_loss(bnn_pass, train_x, wbnn, acts, train_y)\n",
    "        accuracy = get_accuracy(train_y, pred[:,:10])\n",
    "\n",
    "        print(\"step {} loss = {:.2f} accuracy = {:.2f}\".format(epoch, loss, accuracy))\n",
    "        \n",
    "        pred = bnn_pass(val_x, wbnn, acts)\n",
    "        loss = get_log_loss(bnn_pass, val_x, wbnn, acts, val_y)\n",
    "        accuracy = get_accuracy(val_y, pred[:,:10])\n",
    "\n",
    "        print(\"step {} validation loss = {:.2f} accuracy = {:.2f}\".format(epoch, loss, accuracy))\n",
    "        print(lr)\n",
    "    \n",
    "    grads = gradloss(bnn_pass, train_x, wbnn, acts, train_y)\n",
    "    \n",
    "    update, m, v = adam_update(grads, m, v)\n",
    "    \n",
    "    if epoch > 500:\n",
    "    \n",
    "        l2_reg = 1e-9\n",
    "        l1_reg = 1e-9\n",
    "    \n",
    "    for my_grad, my_params in zip(update, wbnn):\n",
    "\n",
    "        my_params -= lr * (my_grad + l1_reg * np.abs(my_params) + l2_reg * np.abs(my_params)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for epoch in range(100):\n",
    "lr = 3e-3\n",
    "\n",
    "wnn = [npr.randn(64,64), npr.randn(64,64), npr.randn(64,64), npr.randn(64,64)]\n",
    "acts = [swish, swish, swish, softmax]\n",
    "\n",
    "gradloss = grad(get_log_loss, argnum=2)\n",
    "\n",
    "l2_reg = 1e-9\n",
    "l1_reg = 1e-9\n",
    "\n",
    "for epoch in range(1000):\n",
    "    \n",
    "    grads = gradloss(nn_pass, train_x, wnn, acts, train_y)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        lr = lr * 0.9\n",
    "        pred = nn_pass(train_x, wnn, acts)\n",
    "        loss = get_log_loss(nn_pass, train_x, wnn, acts, train_y)\n",
    "        accuracy = get_accuracy(train_y, pred[:,:10])\n",
    "\n",
    "        print(\"step {} loss = {:.2f} accuracy = {:.2f}\".format(epoch, loss, accuracy))\n",
    "        \n",
    "        pred = nn_pass(val_x, wnn, acts)\n",
    "        loss = get_log_loss(nn_pass, val_x, wnn, acts, val_y)\n",
    "        accuracy = get_accuracy(val_y, pred[:,:10])\n",
    "\n",
    "        print(\"step {} validation loss = {:.2f} accuracy = {:.2f}\".format(epoch, loss, accuracy))\n",
    "        print(lr)\n",
    "        \n",
    "    if epoch > 200:\n",
    "    \n",
    "        l2_reg = 1e-3\n",
    "        l1_reg = 1e-3\n",
    "        \n",
    "    for my_grad, my_params in zip(grads, wnn):\n",
    "\n",
    "        my_params -= lr * (my_grad + l1_reg * np.abs(my_params) + l2_reg * np.abs(my_params)**2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wbnn = [npr.randn(32,32), npr.randn(32,32), npr.randn(32,32)]\n",
    "acts = [np.tanh, np.tanh, softmax]\n",
    "\n",
    "pred = bnn_pass(train_x, wbnn, acts)\n",
    "loss = get_log_loss(bnn_pass, train_x, wbnn, acts, train_y)\n",
    "accuracy = get_accuracy(train_y, pred)\n",
    "print(loss, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred[0:1, 0:10])\n",
    "\n",
    "log_loss(train_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(pred[0:10,0:10], axis=1))\n",
    "print(np.argmax(train_y[0:10], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_x[0].reshape(8,8))\n",
    "plt.title(\"{}\".format(np.argmax(train_y[0])))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(softmax(pred[0:12,:]),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = npr.randn(1,4)\n",
    "#x[:,:2] *= 0\n",
    "\n",
    "weights = [npr.randn(2,2)] * 4\n",
    "acts = [np.tanh] * 3 + [softmax]\n",
    "flips = [1,0,1,0]\n",
    "\n",
    "temp = bnn_pass(x, weights, acts, flips)\n",
    "\n",
    "temp2 = bnn_pass(temp, weights, acts, flips, forward=False)\n",
    "print(x)\n",
    "print(temp)\n",
    "print(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = npr.randn(1,4)\n",
    "\n",
    "weights = [npr.randn(4,4)] * 4\n",
    "acts = [np.tanh] * 3 + [softmax]\n",
    "flips = [1,0,1,0]\n",
    "\n",
    "temp = nn_pass(x, weights, acts)\n",
    "\n",
    "temp2 = nn_pass(temp, weights, acts, forward=False)\n",
    "\n",
    "print(x)\n",
    "print(temp)\n",
    "print(temp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = ([\"cat\"] *3 + [\"dog\"])\n",
    "temp.reverse()\n",
    "print(temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
